{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04efa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import os\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "from math import floor, sqrt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "# Problem 1\n",
    "class Question:\n",
    "    \"\"\"Questions to use in construction and display of Decision Trees.\n",
    "    Attributes:\n",
    "        column (int): which column of the data this question asks\n",
    "        value (int/float): value the question asks about\n",
    "        features (str): name of the feature asked about\n",
    "    Methods:\n",
    "        match: returns boolean of if a given sample answered T/F\"\"\"\n",
    "    \n",
    "    def __init__(self, column, value, feature_names):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.features = feature_names[self.column]\n",
    "    \n",
    "    def match(self,sample):\n",
    "        \"\"\"Returns T/F depending on how the sample answers the question\n",
    "        Parameters:\n",
    "            sample ((n,), ndarray): New sample to classify\n",
    "        Returns:\n",
    "            (bool): How the sample compares to the question\"\"\"\n",
    "        #compare the sample to the value at the correct column\n",
    "\n",
    "        return sample[self.column] >= self.value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Is %s >= %s?\" % (self.features, str(self.value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bca2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(data,question):\n",
    "    \"\"\"Splits the data into left (true) and right (false)\n",
    "    Parameters:\n",
    "        data ((m,n), ndarray): data to partition\n",
    "        question (Question): question to split on\n",
    "    Returns:\n",
    "        left ((j,n), ndarray): Portion of the data matching the question\n",
    "        right ((m-j, n), ndarray): Portion of the data NOT matching the question\n",
    "    \"\"\"\n",
    "    #use our question method to answer the question and portion accordingly\n",
    "    left = data[data[:, question.column] >= question.value, :]\n",
    "    right = data[data[:, question.column] < question.value, :]\n",
    "\n",
    "    return left, right\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec9d9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2    \n",
    "def gini(data):\n",
    "    \"\"\"Return the Gini impurity of given array of data.\n",
    "    Parameters:\n",
    "        data (ndarray): data to examine\n",
    "    Returns:\n",
    "        (float): Gini impurity of the data\"\"\"\n",
    "    #get the number of samples\n",
    "    nsamples = len(data)\n",
    "    \n",
    "    #get the occurences of each label\n",
    "    label_occurrences = {}\n",
    "    for label in data[:, -1]:\n",
    "        if label in label_occurrences:\n",
    "            label_occurrences[label] += 1\n",
    "        else:\n",
    "            label_occurrences[label] = 1\n",
    "    #calculate the gini index\n",
    "    gini = 1\n",
    "    for occurrence in label_occurrences.values():\n",
    "        gini -= (occurrence/nsamples)**2\n",
    "    \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39202c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left,right,G):\n",
    "    \"\"\"Return the info gain of a partition of data.\n",
    "    Parameters:\n",
    "        left (ndarray): left split of data\n",
    "        right (ndarray): right split of data\n",
    "        G (float): Gini impurity of unsplit data\n",
    "    Returns:\n",
    "        (float): info gain of the data\"\"\"\n",
    "    #get the number of data points in each partition and in total\n",
    "    lp, rp = len(left), len(right)\n",
    "    alltogether = lp+rp\n",
    "    #calculate the summation term for each i\n",
    "    first_term = (lp/alltogether)*gini(left)\n",
    "    second_term = (rp/alltogether)*gini(right)\n",
    "\n",
    "    #return the summation value\n",
    "    return (G - (first_term + second_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a09270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4758\n",
      "0.14580000000000004\n"
     ]
    }
   ],
   "source": [
    "def test_gains():\n",
    "    \"Test the information gain of a partition of data\"\n",
    "    animals = np.loadtxt('animals.csv', delimiter=',')\n",
    "    print(gini(animals))\n",
    "\n",
    "    print(info_gain(animals[:50], animals[50:], gini(animals)))\n",
    "test_gains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c731572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3, Problem 7\n",
    "def find_best_split(data, feature_names, min_samples_leaf=5, random_subset=False):\n",
    "    \"\"\"Find the optimal split\n",
    "    Parameters:\n",
    "        data (ndarray): Data in question\n",
    "        feature_names (list of strings): Labels for each column of data\n",
    "        min_samples_leaf (int): minimum number of samples per leaf\n",
    "        random_subset (bool): for Problem 7\n",
    "    Returns:\n",
    "        (float): Best info gain\n",
    "        (Question): Best question\"\"\"\n",
    "    #get the best gains and question\n",
    "    best_gains = 0\n",
    "    best_question = None\n",
    "\n",
    "    \n",
    "    options = feature_names[:-1]\n",
    "    nfeatures = len(options)\n",
    "    G = gini(data)\n",
    "    #account for random_subset\n",
    "    if random_subset:\n",
    "        n = len(options)\n",
    "        randos = floor(sqrt(n))\n",
    "        rando_ind = np.random.randint(low=0, high=len(options), size=randos)\n",
    "\n",
    "    for col in range(nfeatures):\n",
    "        if random_subset and col not in rando_ind:\n",
    "            continue\n",
    "        split_ops = np.unique(data[:, col])\n",
    "        #get the unique classification options\n",
    "        for op in split_ops:\n",
    "            split_q = Question(column=col, value=op, feature_names=options)\n",
    "            left, right = partition(data, split_q)\n",
    "\n",
    "            if len(left) < min_samples_leaf or len(right) < min_samples_leaf:\n",
    "                continue\n",
    "            \n",
    "            #calculate the information gain of each option\n",
    "            gains = info_gain(left, right, G)\n",
    "            if gains > best_gains:\n",
    "                best_gains = gains\n",
    "                best_question = split_q\n",
    "\n",
    "    return best_gains, best_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efcfac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.12259833679833687, Is # legs/tentacles >= 2.0?)\n"
     ]
    }
   ],
   "source": [
    "def test_best_split():\n",
    "    \"Find the optimal split for the data that maximizes information gain and give the gain\"\n",
    "    # Load in the data\n",
    "    animals = np.loadtxt('animals.csv', delimiter=',')\n",
    "    # Load in feature names\n",
    "    features = np.loadtxt('animal_features.csv', delimiter=',', dtype=str, comments=None)\n",
    "    # Load in sample names\n",
    "    names = np.loadtxt('animal_names.csv', delimiter=',', dtype=str)\n",
    "    print(find_best_split(animals, features))\n",
    "test_best_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e89514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4\n",
    "class Leaf:\n",
    "    \"\"\"Tree leaf node\n",
    "    Attribute:\n",
    "        prediction (dict): Dictionary of labels at the leaf\"\"\"\n",
    "    def __init__(self,data):\n",
    "\n",
    "        #account for there only being one entry\n",
    "        if len(data.shape) == 1:\n",
    "            predict = {data[-1] : 1}\n",
    "            self.prediction = predict\n",
    "        else:\n",
    "            label_occurr = {}\n",
    "            #count the number of times each label appears\n",
    "            for label in data[:,-1]:\n",
    "                if label in label_occurr:\n",
    "                    label_occurr[label] += 1\n",
    "                else:\n",
    "                    label_occurr[label] = 1  \n",
    "            \n",
    "            self.prediction = label_occurr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3996a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Node:\n",
    "    \"\"\"Tree node with a question\n",
    "    Attributes:\n",
    "        question (Question): Question associated with node\n",
    "        left (Decision_Node or Leaf): child branch\n",
    "        right (Decision_Node or Leaf): child branch\"\"\"\n",
    "    def __init__(self, question, left_branch, right_branch):\n",
    "        #initialize the attributes\n",
    "        self.question = question\n",
    "        self.left_branch = left_branch\n",
    "        self.right_branch = right_branch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "962fd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to draw a tree\n",
    "def draw_node(graph, my_tree):\n",
    "    \"\"\"Helper function for drawTree\"\"\"\n",
    "    node_id = uuid4().hex\n",
    "    #If it's a leaf, draw an oval and label with the prediction\n",
    "    if isinstance(my_tree, Leaf):\n",
    "        graph.node(node_id, shape=\"oval\", label=\"%s\" % my_tree.prediction)\n",
    "        return node_id\n",
    "    else: #If it's not a leaf, make a question box\n",
    "        graph.node(node_id, shape=\"box\", label=\"%s\" % my_tree.question)\n",
    "        left_id = draw_node(graph, my_tree.left)\n",
    "        graph.edge(node_id, left_id, label=\"T\")\n",
    "        right_id = draw_node(graph, my_tree.right)    \n",
    "        graph.edge(node_id, right_id, label=\"F\")\n",
    "        return node_id\n",
    "\n",
    "def draw_tree(my_tree):\n",
    "    \"\"\"Draws a tree\"\"\"\n",
    "    #Remove the files if they already exist\n",
    "    for file in ['Digraph.gv','Digraph.gv.pdf']:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "    graph = graphviz.Digraph(comment=\"Decision Tree\")\n",
    "    draw_node(graph, my_tree)\n",
    "    graph.render(view=True) #This saves Digraph.gv and Digraph.gv.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3943c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prolem 5\n",
    "def build_tree(data, feature_names, min_samples_leaf=5, max_depth=4, current_depth=0, random_subset=False):\n",
    "    \"\"\"Build a classification tree using the classes Decision_Node and Leaf\n",
    "    Parameters:\n",
    "        data (ndarray)\n",
    "        feature_names(list or array)\n",
    "        min_samples_leaf (int): minimum allowed number of samples per leaf\n",
    "        max_depth (int): maximum allowed depth\n",
    "        current_depth (int): depth counter\n",
    "        random_subset (bool): whether or not to train on a random subset of features\n",
    "    Returns:\n",
    "        Decision_Node (or Leaf)\"\"\"\n",
    "    #if another split will takes us below our min samples, end the recursion\n",
    "    if len(data) < 2 * min_samples_leaf:\n",
    "        return Leaf(data)\n",
    "    \n",
    "    #iterate recursively\n",
    "    best_gains, best_question = find_best_split(data, feature_names, min_samples_leaf=min_samples_leaf, random_subset=random_subset)\n",
    "\n",
    "    #end if we don't improve or we are at max depth\n",
    "    if best_gains == 0 or current_depth >= max_depth:\n",
    "        return Leaf(data)\n",
    "\n",
    "    left, right = partition(data, best_question)\n",
    "    \n",
    "    #split and build nodes for each side if we do split\n",
    "    leftie = build_tree(left, feature_names,min_samples_leaf=min_samples_leaf, max_depth=max_depth, current_depth=current_depth+1, random_subset=random_subset)\n",
    "    rightie = build_tree(right, feature_names, min_samples_leaf=min_samples_leaf, max_depth=max_depth, current_depth=current_depth+1, random_subset=random_subset)\n",
    "\n",
    "    return Decision_Node(best_question, leftie, rightie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58c5b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 6\n",
    "def predict_tree(sample, my_tree):\n",
    "    \"\"\"Predict the label for a sample given a pre-made decision tree\n",
    "    Parameters:\n",
    "        sample (ndarray): a single sample\n",
    "        my_tree (Decision_Node or Leaf): a decision tree\n",
    "    Returns:\n",
    "        Label to be assigned to new sample\"\"\"\n",
    "\n",
    "    #end the recursion if we are at a leaf\n",
    "    if isinstance(my_tree, Leaf):\n",
    "        return max(my_tree.prediction, key=my_tree.prediction.get)\n",
    "\n",
    "    #continue by recursion at nodes and parse through tree based upon each node's decision\n",
    "    if my_tree.question.match(sample):\n",
    "        return predict_tree(sample, my_tree.left_branch)\n",
    "        \n",
    "    else:\n",
    "        return predict_tree(sample, my_tree.right_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5cf941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tree(dataset,my_tree):\n",
    "    \"\"\"Test how accurately a tree classifies a dataset\n",
    "    Parameters:\n",
    "        dataset (ndarray): Labeled data with the labels in the last column\n",
    "        tree (Decision_Node or Leaf): a decision tree\n",
    "    Returns:\n",
    "        (float): Proportion of dataset classified correctly\"\"\"\n",
    "\n",
    "    nsamples = len(dataset)\n",
    "    ngoodies = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        #see if the prediction is correct\n",
    "        if predict_tree(sample, my_tree) == sample[-1]:\n",
    "            ngoodies += 1\n",
    "        else:\n",
    "            pass\n",
    "    #calculate percentage of correct labels\n",
    "    return ngoodies / nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "964fee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "def test_tree():\n",
    "    # Load in the data\n",
    "    animals = np.loadtxt('animals.csv', delimiter=',')\n",
    "    # Load in feature names\n",
    "    animal_features = np.loadtxt('animal_features.csv', delimiter=',', dtype=str, comments=None)\n",
    "    # Load in sample names\n",
    "    names = np.loadtxt('animal_names.csv', delimiter=',', dtype=str)\n",
    "    np.random.shuffle(animals)\n",
    "    training_set, test_set = animals[:80, :], animals[80:, :]\n",
    "    my_tree = build_tree(data=training_set, feature_names=animal_features)\n",
    "    print(analyze_tree(dataset=test_set, my_tree=my_tree))\n",
    "test_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52108f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 7\n",
    "def predict_forest(sample, forest):\n",
    "    \"\"\"Predict the label for a new sample, given a random forest\n",
    "    Parameters:\n",
    "        sample (ndarray): a single sample\n",
    "        forest (list): a list of decision trees\n",
    "    Returns:\n",
    "        Label to be assigned to new sample\"\"\"\n",
    "    \n",
    "    #get the prediction of each tree in the forest\n",
    "    tree_labels = [predict_tree(sample, tree) for tree in forest]\n",
    "\n",
    "    #get the most common label\n",
    "    return max(set(tree_labels), key=tree_labels.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f13eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_forest(dataset,forest):\n",
    "    \"\"\"Test how accurately a forest classifies a dataset\n",
    "    Parameters:\n",
    "        dataset (ndarray): Labeled data with the labels in the last column\n",
    "        forest (list): list of decision trees\n",
    "    Returns:\n",
    "        (float): Proportion of dataset classified correctly\"\"\"\n",
    "    nsamples = len(dataset)\n",
    "    goodies = 0\n",
    "    for sample in dataset:\n",
    "        if predict_forest(sample, forest) == sample[-1]:\n",
    "            #see if our forest predicted correctly or not\n",
    "            good = 1\n",
    "        else:\n",
    "            good = 0\n",
    "        goodies += good\n",
    "\n",
    "    #get the percentage of samples labeled correctly\n",
    "    return goodies / nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d8b8014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.8, 0.11325216293334961),\n",
       " (0.7333333333333333, 0.0045659542083740234),\n",
       " (0.7355371900826446, 0.22952008247375488))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem 8\n",
    "def prob8():\n",
    "    \"\"\"Use the file parkinsons.csv to analyze a 5 tree forest.\n",
    "    \n",
    "    Create a forest with 5 trees and train on 100 random samples from the dataset.\n",
    "    Use 30 random samples to test using analyze_forest() and SkLearn's \n",
    "    RandomForestClassifier.\n",
    "    \n",
    "    Create a 5 tree forest using 80% of the dataset and analzye using \n",
    "    RandomForestClassifier.\n",
    "    \n",
    "    Return three tuples, one for each test.\n",
    "    \n",
    "    Each tuple should include the accuracy and time to run: (accuracy, running time) \n",
    "    \"\"\"\n",
    "    #load the data\n",
    "    park_dat = np.loadtxt('parkinsons.csv', delimiter=',', dtype=float, comments=None)\n",
    "    park_feats = np.loadtxt('parkinsons_features.csv', delimiter=',', dtype=str, comments=None)\n",
    "\n",
    "    #don't use participant ID\n",
    "    park_dat = park_dat[:, 1:]\n",
    "    park_feats = park_feats[1:]\n",
    "\n",
    "    #shuffle the data\n",
    "    np.random.shuffle(park_dat)\n",
    "\n",
    "    #use a subset\n",
    "    train = park_dat[:100, :]\n",
    "    test = park_dat[100:130, :]\n",
    "\n",
    "    #use our forest class that we created on subset\n",
    "    class_start= time.time()\n",
    "    classy_forest = [build_tree(data=train, feature_names=park_feats, min_samples_leaf=15,max_depth=4, random_subset=True) for i in range(5)]\n",
    "    class_acc = analyze_forest(dataset=test, forest=classy_forest)\n",
    "    class_time = time.time() - class_start\n",
    "\n",
    "    #use sklearn on subset\n",
    "    sklearn_start = time.time()\n",
    "    learned_forest = RandomForestClassifier(n_estimators=5, max_depth=4, min_samples_leaf=15)\n",
    "    learned_forest.fit(train[:, :-1], train[:, -1])\n",
    "    learned_acc = learned_forest.score(test[:, :-1], test[:, -1])\n",
    "    learned_time = time.time() - sklearn_start\n",
    "\n",
    "    #use sklearn on whole thing\n",
    "    nsamples = len(park_dat)\n",
    "    trainees = floor(nsamples*.8)\n",
    "    train, test = park_dat[:trainees, :], park_dat[trainees:, :]\n",
    "    bigsklearn_start = time.time()\n",
    "    bigsklearn = RandomForestClassifier()\n",
    "    bigsklearn.fit(train[:, :-1], train[:, -1])\n",
    "    bigsklearn_acc = bigsklearn.score(test[:, :-1], test[:, -1])\n",
    "    bigsklearn_time = time.time() - bigsklearn_start\n",
    "\n",
    "    return (class_acc, class_time), (learned_acc, learned_time), (bigsklearn_acc, bigsklearn_time)\n",
    "prob8()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
